{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "This notebook shows how to perfrom a full training of the PlaNet model using pytorch `lightning`. Before running it, please make sure to have `planet` installed. See the README.md file.\n",
    "\n",
    "This consists in the following steps:\n",
    "1. instantiate the model and the datamodule, as well some useful callbacks;\n",
    "2. train the model, logging the training status on Weights and Biases,\n",
    "3. save the model and the related data (config file and scaler) to perform inference\n",
    "\n",
    "You can training the model as well by running the `make train` command in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from planet.config import Config\n",
    "from planet.train import main_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** to use Weights and Biases, you need a valid account and to be logged in. To log in, run the following command in your terminal\n",
    "\n",
    "```shell\n",
    "wandb login --relogin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of the box training\n",
    "\n",
    "A full training can be run bu wunning these 2 commands. First define the a `Config` object with all the confiruration. You can also create a `config.yml` file like [this one](../config/config.yml) and load it using `planet.utils.load_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    save_path=  'trained_models/test/', #path where to save the model\n",
    "    dataset_path= 'planet_sample_dataset.h5', # path to you dataset (see notebook 1_dataset_creation.ipynb)\n",
    "    is_physics_informed = True, # if compute also the physics informes term in the loss funciton\n",
    "    do_super_resolution= False, # to do super-resolution: Very expensive! if True, num_workers should be ~batch_size//2\n",
    "    batch_size= 16, # training batch size\n",
    "    epochs= 10, # training epochs\n",
    "    log_to_wandb= True, # if true, logs to wand\n",
    "    wandb_project= 'planet_test', # wandb project name\n",
    "    save_checkpoints= True, # if true, saves checkpoint at best eval loss, keep last 2\n",
    "    resume_from_checkpoint= False, #if true, the training is resumed from the last checkpoint\n",
    "    num_workers= 0, # num workest in the dataloarer. If ==-1, the value is set automatically\n",
    "    planet_config={ \n",
    "        'hidden_dim' : 128, \n",
    "        'nr' : 64, # this must match with the nr in your input grids\n",
    "        'nz' : 64, # this must match with the nz in your input grids\n",
    "        'n_measures' : 302, # this must match with the total input dimension (see notebook 1_dataset_creation.ipynb)\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the `main_train` script, that will do all the points 1 -> 3 and store the model in `config.save_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteob-90-hotmail-it\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250519_193520-oujn6m2v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteob-90-hotmail-it/planet_test/runs/oujn6m2v' target=\"_blank\">hopeful-smoke-51</a></strong> to <a href='https://wandb.ai/matteob-90-hotmail-it/planet_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteob-90-hotmail-it/planet_test' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/planet_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteob-90-hotmail-it/planet_test/runs/oujn6m2v' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/planet_test/runs/oujn6m2v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | PlaNetCore | 1.8 M  | train\n",
      "1 | loss_module | PlaNetLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.121     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 13.01it/s, v_num=6m2v, train_loss=26.40, val_loss=30.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.73it/s, v_num=6m2v, train_loss=26.40, val_loss=30.40]\n",
      "Loading best model from checkpoint: trained_models/test/ckp/epoch=9-step=40.ckpt\n"
     ]
    }
   ],
   "source": [
    "main_train(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training\n",
    "The following cell is the content of the `main_train` function. You can edit it in any place to perform any kind of customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/notebooks/trained_models/ckp exists and is not empty.\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | PlaNetCore | 1.8 M  | train\n",
      "1 | loss_module | PlaNetLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.121     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 16.32it/s, v_num=vryl, train_loss=57.90, val_loss=53.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 11.63it/s, v_num=vryl, train_loss=57.90, val_loss=53.90]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, Callback\n",
    "from lightning.pytorch.loggers import WandbLogger, Logger\n",
    "\n",
    "from planet.train import LightningPlaNet, DataModule\n",
    "from planet.utils import get_accelerator, last_ckp_path, save_model_and_scaler\n",
    "\n",
    "save_dir = Path(config.save_path)\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "### instantiate model and datamodule\n",
    "model = LightningPlaNet(config=config)\n",
    "datamodule = DataModule(config=config)\n",
    "\n",
    "### define some callbacks\n",
    "callbacks = []\n",
    "if config.save_checkpoints is not None:\n",
    "    callbacks.append(\n",
    "        ModelCheckpoint(\n",
    "            dirpath=save_dir / Path(\"ckp\"), save_top_k=2, monitor=\"val_loss\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# get the logger\n",
    "logger = None\n",
    "if config.log_to_wandb:\n",
    "    logger = WandbLogger(project=config.wandb_project)\n",
    "\n",
    "### train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator=get_accelerator(),\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=(\n",
    "        last_ckp_path(save_dir / Path(\"ckp\"))\n",
    "        if config.resume_from_checkpoint\n",
    "        else None\n",
    "    ),\n",
    ")\n",
    "\n",
    "### save model + scaler for inference\n",
    "save_model_and_scaler(trainer, datamodule.dataset.scaler, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
