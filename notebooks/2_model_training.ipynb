{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "This notebook shows how to perfrom a full training of the PlaNet model using pytorch `lightning`. Before running it, please make sure to have `planet` installed. See the README.md file.\n",
    "\n",
    "This consists in the following steps:\n",
    "1. instantiate the model and the datamodule, as well some useful callbacks;\n",
    "2. train the model, logging the training status on Weights and Biases,\n",
    "3. save the model and the related data (config file and scaler) to perform inference\n",
    "\n",
    "You can training the model as well by running the `make train` command in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from planet.config import Config\n",
    "from planet.train import main_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** to use Weights and Biases, you need a valid account and to be logged in. To log in, run the following command in your terminal\n",
    "\n",
    "```shell\n",
    "wandb login --relogin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of the box training\n",
    "\n",
    "A full training can be run bu wunning these 2 commands. First define the a `Config` object with all the confiruration. You can also create a `config.yml` file like [this one](../config/config.yml) and load it using `planet.utils.load_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    save_path=  'trained_models/test/', #path where to save the model\n",
    "    dataset_path= 'planet_sample_dataset.h5', # path to you dataset (see notebook 1_dataset_creation.ipynb)\n",
    "    is_physics_informed = True, # if compute also the physics informes term in the loss funciton\n",
    "    do_super_resolution= False, # to do super-resolution: Very expensive! if True, num_workers should be ~batch_size//2\n",
    "    batch_size= 16, # training batch size\n",
    "    epochs= 10, # training epochs\n",
    "    log_to_wandb= True, # if true, logs to wand\n",
    "    wandb_project= 'planet_test', # wandb project name\n",
    "    save_checkpoints= True, # if true, saves checkpoint at best eval loss, keep last 2\n",
    "    resume_from_checkpoint= False, #if true, the training is resumed from the last checkpoint\n",
    "    num_workers= 0, # num workest in the dataloarer. If ==-1, the value is set automatically\n",
    "    planet_config={ \n",
    "        'hidden_dim' : 128, \n",
    "        'nr' : 64, # this must match with the nr in your input grids\n",
    "        'nz' : 64, # this must match with the nz in your input grids\n",
    "        'n_measures' : 302, # this must match with the total input dimension (see notebook 1_dataset_creation.ipynb)\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the `main_train` script, that will do all the points 1 -> 3 and store the model in `config.save_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'coils_current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RESEARCH/PlaNet_Equil_reconstruction/planet/train.py:148\u001b[0m, in \u001b[0;36mmain_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m### instantiate model and datamodule\u001b[39;00m\n\u001b[1;32m    147\u001b[0m model \u001b[38;5;241m=\u001b[39m LightningPlaNet(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m--> 148\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m DataModule(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m### define some callbacks\u001b[39;00m\n\u001b[1;32m    151\u001b[0m callbacks: List[Callback] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/RESEARCH/PlaNet_Equil_reconstruction/planet/train.py:41\u001b[0m, in \u001b[0;36mDataModule.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mplanet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust provide valid config.planet, got None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mPlaNetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_physics_informed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_physics_informed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplanet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplanet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_super_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_super_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     50\u001b[0m     cpu_count() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/RESEARCH/PlaNet_Equil_reconstruction/planet/data.py:162\u001b[0m, in \u001b[0;36mPlaNetDataset.__init__\u001b[0;34m(self, path, dtype, is_physics_informed, nr, nz, do_super_resolution)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_super_resolution \u001b[38;5;241m=\u001b[39m do_super_resolution\n\u001b[1;32m    159\u001b[0m data \u001b[38;5;241m=\u001b[39m read_h5_numpy(path)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit_transform(\n\u001b[1;32m    161\u001b[0m     np\u001b[38;5;241m.\u001b[39mcolumn_stack(\n\u001b[0;32m--> 162\u001b[0m         (data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasures\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoils_current\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_profile\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflux \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflux\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrhs \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrhs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'coils_current'"
     ]
    }
   ],
   "source": [
    "main_train(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training\n",
    "The following cell is the content of the `main_train` function. You can edit it in any place to perform any kind of customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/notebooks/trained_models/ckp exists and is not empty.\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | PlaNetCore | 1.8 M  | train\n",
      "1 | loss_module | PlaNetLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.121     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/matte/Documents/RESEARCH/PlaNet_Equil_reconstruction/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 16.32it/s, v_num=vryl, train_loss=57.90, val_loss=53.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 11.63it/s, v_num=vryl, train_loss=57.90, val_loss=53.90]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, Callback\n",
    "from lightning.pytorch.loggers import WandbLogger, Logger\n",
    "\n",
    "from planet.train import LightningPlaNet, DataModule\n",
    "from planet.utils import get_accelerator, last_ckp_path, save_model_and_scaler\n",
    "\n",
    "save_dir = Path(config.save_path)\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "### instantiate model and datamodule\n",
    "model = LightningPlaNet(config=config)\n",
    "datamodule = DataModule(config=config)\n",
    "\n",
    "### define some callbacks\n",
    "callbacks = []\n",
    "if config.save_checkpoints is not None:\n",
    "    callbacks.append(\n",
    "        ModelCheckpoint(\n",
    "            dirpath=save_dir / Path(\"ckp\"), save_top_k=2, monitor=\"val_loss\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# get the logger\n",
    "logger = None\n",
    "if config.log_to_wandb:\n",
    "    logger = WandbLogger(project=config.wandb_project)\n",
    "\n",
    "### train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator=get_accelerator(),\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=(\n",
    "        last_ckp_path(save_dir / Path(\"ckp\"))\n",
    "        if config.resume_from_checkpoint\n",
    "        else None\n",
    "    ),\n",
    ")\n",
    "\n",
    "### save model + scaler for inference\n",
    "save_model_and_scaler(trainer, datamodule.dataset.scaler, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
